{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f91b088-5f62-46ef-bf83-06e9b1daed35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Lab 2 Key: Data Wrangling and Exploratory Data Analysis\n",
    "Now that you have the basics down let's start looking at some real data! After completing this lab you will be able to:\n",
    "- Import data into a pandas `Data Frame`\n",
    "- Read and use the pandas documentation to implement a function\n",
    "- Merge datasets using common fields\n",
    "- Filter data\n",
    "- Calculate new columns\n",
    "- Transform data from wide to long and long to wide\n",
    "- Create some simple charts\n",
    "- Sort data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656665e-2507-40b3-811e-df98f6b47c95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Some if this should look familiar after completing lab number 1. Refer back the the previous lab as well as to the pandas [cheatsheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) or [documentation](https://pandas.pydata.org/docs/index.html) as you run into trouble. \n",
    "\n",
    "For this lab we will be using UDOT pavement data from:\n",
    "- Sections Data Source: https://data-uplan.opendata.arcgis.com/datasets/historic-pavement-section-data-open-data/\n",
    "- Attributes Data Source: https://maps.udot.utah.gov/arcgis/rest/services/Complex/PAVE_PaveMgmtLevelHistory/MapServer/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1778d-bd9d-4b03-b299-613558132fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only run this code if using Jupyter Lite ###\n",
    "#Installs plotting library\n",
    "%pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e4b87-2536-4692-91ec-6eaec4ba0291",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Numpy is a package with additional math and data functions\n",
    "import numpy as np\n",
    "\n",
    "#Matplotlib and Seaborn are data visualizaiton packages that we will use for plotting down the line\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d2323-8827-4f72-bf61-9053ed41cfb7",
   "metadata": {},
   "source": [
    "## Reading and Merging the Data\n",
    "First we need to read the data as a dataframe. Here is an example reading in the sections data, look at then then try reading in the attribute table yourself! You'll also see us use the `.head()` method which displays the first n rows of the data frame (with five rows being the default). **Hint:** `.tail()` will display the last n rows of a data frame (with 5 as the default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f2c26-0b1a-4489-887d-ac5ca6573850",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import sections dataset from the relative file path (the pavement_section_data.csv file located in the data folder)\n",
    "#Note Filepath is different from primary lab\n",
    "sections = pd.read_csv('../data/pavement_section_data.csv')\n",
    "\n",
    "#Display the first 5 rows of `sections`\n",
    "sections.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bffc9d-803c-4d28-9cf0-69dc9990f203",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Import the attributes dataset located at <code>'data/pavement_attributes.csv'</code> into a variable named <code>attrs</code> and display the first five rows</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93308f7-2235-4330-b037-14db629808bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: Filepath is different from primary lab\n",
    "attrs = pd.read_csv('../data/pavement_attributes.csv') ### Your Code Here ###\n",
    "attrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97480956-f392-4632-8caf-d0826b9fc9dc",
   "metadata": {},
   "source": [
    "Now that we have our data frames we need to combine them into one data frame with both sets of columns. We can do this using the `pd.merge()` function (think of it like a SQL `JOIN` or an Excel `VLOOKUP`). To get started with this function take a look at its [documentation](https://pandas.pydata.org/docs/reference/api/pandas.merge.html). While reading the documentation for a package can appear daunting at first it often tells you much that you need to know to implement a new tool. Typically you can see the function name and the arguments it accepts as well as the defaults for each argument. As you scroll down the page you can see a more in depth description of each argument and the options available for it as well as some examples. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Create a new variable called <code>df</code> using the <code>pd.merge()</code>function that uses <code>sections</code> as the left dataframe and <code>attrs</code> as the right. Use the list of common keys defined below as the keys to match on. Use the documentation and examples for help.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e41c0-7cfd-4acf-b1b9-fd70c373d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of key collumns included in both datarames\n",
    "common_keys = [\"ROUTE\", \"DIR\", \"LOCATION\", \"REGION\"]\n",
    "\n",
    "df = pd.merge(sections, attrs, on=common_keys)\n",
    "\n",
    "# Display the head of the Dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c476e-51c8-4701-9497-9dea3f052a6f",
   "metadata": {},
   "source": [
    "If you're having trouble click the drop down arrow to see the answer:\n",
    "<details>\n",
    "    <summary>Click here to hide/unhide the answer!</summary>\n",
    "  \n",
    "  ### Answer\n",
    "  ```python\n",
    "df = pd.merge(sections, attrs, on=common_keys)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bcdeb-f7f6-42d1-8217-99d0564efbdf",
   "metadata": {},
   "source": [
    "Now that we have the data joined together, let's take a look at the data and do some initial exploration of the data! In the last lab you filtered a data frame, selected specific columns, and got counts of the unique values in individual columns. Let's put those skills to use! Look at the previous lab as a reference and click the hint arrow below as a last resort!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> This is a multi part exercise with a few steps:\n",
    "    <ol>\n",
    "        <li>Get the <code>.value_counts()</code> of the 'SURFACE_TY' column</li>\n",
    "        <li>You should see three records with 'Gravel' as the surface type. Filter for these rows using a condition (<code>'SURFACE_TY' == 'Gravel'</code>) so we can take a closer look</li>\n",
    "        <li>There are a lot of columns to look at so lets look at a smaller selection of columns, specifically <code>'ROUTE'</code>, <code>'LOCATION'</code>, <code>'SURFACE_TY'</code>, and <code>'AADT_2011'</code>. <b>Hint:</b> Think first about filtering the rows, then select the columns you are looking for.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90852671-8612-48d9-8324-f9f7fb76383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SURFACE_TY'].value_counts() ### Get the count of unique surface types ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e714c-f1c1-41e9-8997-f1c278436414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['SURFACE_TY']=='Gravel']### Filter for only records where the surface type is 'Gravel' ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbef53-060f-428a-aa01-33022d7866a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['SURFACE_TY']=='Gravel'][['ROUTE', 'LOCATION', 'SURFACE_TY', 'AADT_2011']]### Filter again, then show only the Route, Location, Surface Type, and 2011 AADT fields ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d2f13-062e-4968-b096-39db524ec3e8",
   "metadata": {},
   "source": [
    "If you're having trouble click the drop down arrow to see the answers:\n",
    "<details>\n",
    "    <summary>Click here to hide/unhide the answers!</summary>\n",
    "  \n",
    "  ### Answers\n",
    "  ```python\n",
    "    df['SURFACE_TY'].value_counts()   #Get the count of each type of surface\n",
    "    df[df['SURFACE_TY']=='Gravel']    #Show only records with 'Gravel' as the Surface Type\n",
    "    df[df['SURFACE_TY']=='Gravel'][['ROUTE', 'LOCATION', 'SURFACE_TY', 'AADT_2011']] #Show only the selected columns for records where the surface type is gravel\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389500cf-d92b-417c-86f9-0b30776e25c6",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "Now we need to a little data processing. First things first we need to filter the data so we only use the rows we are interested in looking at. For our purposes we want rows where the surface type is not Gravel that were overlaid (`'YR_OVL'`) before 2013. We've already explored filtering above but in this case we want to filter and overwrite our data frame so df only includes the rows we are interested. Here is an example for removing the segments with gravel as the surface type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a121dd3e-0228-456c-a2df-1b04d328cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the variable df to be equal to df where df['SURFACE_TY'] is not equal to 'Gravel'\n",
    "df = df[df['SURFACE_TY']!='Gravel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44356d1-432c-4b2d-a18f-3a417700dbeb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Your turn! Filter <code>df</code> so that we only keep rows where the <code>'YR_OVL'</code> is before 2013. Take a look at the comparison operators cheat sheet if you need to!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cac64a-ad39-4efd-94e3-97b28b594895",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###\n",
    "df = df[df['YR_OVL']<2013]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e3dff-c49d-4933-b005-d586a6d73784",
   "metadata": {},
   "source": [
    "Next we want to calculate a few columns. Specifically we need a column with a unique ID for each row, and a column with the difference between the year overlaid (`'YR_OVL'`) and year surfaced (`'YR_SURF'`). \n",
    "\n",
    "To create a unique column id we will join the route, direction, and object id fields by applying python's `str.join()` method. To do this we need to first make sure all of the columns are strings, then we can use the pandas `DataFrame.apply()` method to join them together. Let's start by looking at the data types of each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8dcab-6572-4cb2-a7d9-2e9ab3c9abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of the id columns we want to use\n",
    "id_cols = ['ROUTE', 'DIR', 'OBJECTID']\n",
    "#Get the info for these columns\n",
    "df[id_cols].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150040b-d094-4ee2-86b0-9b66a1d8a6a0",
   "metadata": {},
   "source": [
    "It looks like `ROUTE` and `DIR` are already objects (string) data but `OBJECTID` is not. Let's fix that using `.astype(str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23138d5c-ea0e-4d72-bd1e-b9018675ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewrite the object id column with the object id column cast to a string\n",
    "df['OBJECTID'] = df['OBJECTID'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e84be-eddf-49ad-bc5f-7750c8eef5a6",
   "metadata": {},
   "source": [
    "Now that our columns are all objects we can join them. We use the pandas `.apply()` method (which allows you to apply a function along every row of a data frame), and the python `str.join()` method (which concatenates a list of strings using a seperator, in this case, `'-'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd47f36-f771-4407-bf21-98487026c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new column called id that inlcludes the joined id columns\n",
    "df['id'] = df[id_cols].apply('-'.join, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe64c5a-60e2-42a7-b015-796e31875d81",
   "metadata": {},
   "source": [
    "Now for something a little simpler, we want the difference between two columns, You've done something like this in the last lab so try to apply that knowledge here.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Create a new column of the data frame called <code>'gap'</code> that is the difference between the <code>'YR_OVL'</code> and <code>'YR_SURF'</code>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db47f932-54fb-45e5-9457-c289253c3993",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gap'] = df['YR_SURF'] - df['YR_OVL'] ### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53401731-1dad-4eeb-917f-b854394e66e3",
   "metadata": {},
   "source": [
    "## Data Transformations\n",
    "\n",
    "Often, before we can plot or model our data, we need to ensure that our data is in a proper format. This can mean many things depending on our needs, but some of the most common transformations include shifting data from wide to long.\n",
    "\n",
    "<figure style=\"text-align: center;\">\n",
    "  <img src=\"../links/wide-long.png\" alt=\"A graphic displaying data in wide vs long formats\" width=\"600\" style=\"display: block; margin: 0 auto;\">\n",
    "  <figcaption style=\"max-width: 80%; margin: 0 auto;\">Wide data includes a record (row) for every unique item, with observations recorded in separate columns. Long data includes a record for every observation with a single column for the type of observation. Source: <a href=\"https://louisahsmith.github.io/R-office-hours/OH-10.html\">https://louisahsmith.github.io/R-office-hours/OH-10.html</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Long data is often used in data science to look not just at the object being recorded but each metric or observation individually. Often it is the format required for generating certain types of plot or allows for more effective time series analysis. In python we can use the `pandas.melt()` function to transform our data. We do this by identifying the columns to use as ids (the columns we would like to keep without transformation) and value variables (the columns containing each observation that we would like to transform from wide to long)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889ae413-2636-44d3-9ec6-3e485e1625f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the id variables of each record to keep after pivoting.\n",
    "id_vars = ['id', 'SURFACE_TY', 'REGION', 'YR_SURF', 'CLASS', 'MILES', 'AADT_2011']\n",
    "\n",
    "#Define the list of values we would like to unpivot\n",
    "value_vars = ['OCI_04', 'OCI_05', 'OCI_06', 'OCI_07', 'OCI_08', 'OCI_09', 'OCI_10', 'OCI_11', 'OCI_12']\n",
    "\n",
    "### Create and save a long data frame to a new variable  ###\n",
    "long_df = pd.melt(             #For longer functions it can be helpful to break it up on multiple lines\n",
    "    df,\n",
    "    id_vars = id_vars,         #set id_vars keyword to the defined id_vars list we created above\n",
    "    value_vars = value_vars,   #set value_vars keyword to the defined value_vars list we created above\n",
    "    var_name =   'obs_year',   #set the column title for the column defining the variable \n",
    "    value_name = 'OCI'         #set the column title for the value column\n",
    ")\n",
    "\n",
    "#Display the head of long_df\n",
    "long_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf68a9d-e085-4288-b6ba-d385bef1d119",
   "metadata": {},
   "source": [
    "Let's hold on to this new long data frame and come back to it after discussing exploratory data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d169d-5019-4318-b3c8-e1b8b8b987fe",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Now we that we have the data we need in clean and useful format let's begin to explore a bit more about it. You've already been able to see the head and tail of a data frame, as well as the count of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc794fb-2227-4058-9cb1-83e85d98df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See a list of all of the columns in our data frame\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d53f38-5c97-4b43-b25c-fd9b1926819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See some summary statistics about each column in our dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bb38b-0db0-47c1-bcb0-8ebdebf387ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a count of the records for each column by class and region\n",
    "df.groupby(['CLASS', 'REGION']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d18e0ce-5882-47c3-acde-1954e6f67a5c",
   "metadata": {},
   "source": [
    "This is the pandas `DataFrame.groupby()` method. If you are familiar with SQL `GROUPBY` functions or Excel pivot tables this works in a similar way. It allows you to aggregate the data based on one or more variables and perform a variety of calculations such as `.sum()`, `.mean()`, `.count()`, and more! You can even perform different calculations on different columns. If you'd like to look at the more advanced options, look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html#pandas.DataFrame.groupby)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75d63c-6ece-429f-bb55-400a74eca9dd",
   "metadata": {},
   "source": [
    "It is also often helpful to see the distribution of and relationships in data by doing simple visualizations. We'll cover more advanced data visualiztion in lab 3 but for now lets start with the built in histogram functionality in pandas using the `.hist()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca358b0-0c96-43d0-8245-296582915e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the OCI_12 column and create a histogram with 10 bins.\n",
    "df['OCI_12'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab2137-d9a6-4830-b783-f450515078ee",
   "metadata": {},
   "source": [
    "## Your Turn\n",
    "Now it is your turn to do some exploratory data analysis and data cleaning! Go through the series of exercises below on your own to explore the data frames we have created and do some additional processing on them. Let's go back a step and start with our first data frame, `df`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Use the pandas <code>.groupby()</code> method to aggregate the data in <code>df</code>. \n",
    "    <ul>Get:\n",
    "        <li>A count of all segments by surface type (SURFACE_TY) and region (REGION)</li>\n",
    "        <li>The sum of miles (MILES) by the overlay year (YR_OVL) <b>Hint:</b> think about how you select a single column from the data frame and apply that after you sum</li>\n",
    "        <li>The sum of miles by the year surfaced (YR_SURF)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd7af4d-e7cb-407e-9ae1-99a59ef5af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['SURFACE_TY', 'REGION']).size()### Get the count of all segments by surface types and region ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf63a4d-a9fb-4b00-b9f5-a9b215e11264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('YR_OVL').sum()['MILES']### Get the sum of miles by the overlay year ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c673e-8331-45e5-b226-8ddfa81d754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('YR_SURF').sum()['MILES'] ### Get the sum of miles by the year surfaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b5728-90c3-4092-81f5-14660b338e15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Create histograms of the following columns: \n",
    "    <ul>\n",
    "        <li>RIDE_12</li>\n",
    "        <li>RUT_12</li>\n",
    "        <li>FALT_12</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55cfa2-2b3a-4079-9010-255ecede378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RIDE_12'].hist()### Your Code Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45dcc14-d0f5-4773-bf17-53c1ede567da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RUT_12'].hist()### Your Code Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a778dd-d3aa-4f58-a490-5c95b4c1b6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FALT_12'].hist()### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5f6691-90a8-4d2b-89c4-69cce7ef09da",
   "metadata": {},
   "source": [
    "We also often want to see how correlated our data is. This is a little more complicated to plot out but take a look at the possiblities and come back to this after you've done Lab 3 to put together what is happening here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836bd65-9ec8-47b2-bc5f-c7116d75388d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plots a correlation matrix for the dataframe.\n",
    "\n",
    "#Choose the variables to check correlation\n",
    "vars = ['OCI_12', 'RIDE_12', 'RUT_12', 'FALT_12']\n",
    "\n",
    "#Set the Figure Size to 5 wide and 5 tall\n",
    "fig, ax = plt.subplots(figsize=(5, 5)) \n",
    "\n",
    "#Create a correlation matrix of vars\n",
    "corr_matrix = df[vars].corr() \n",
    "\n",
    "#View correlation matrix as a seaborn heatmap\n",
    "sns.heatmap(corr_matrix, cmap='RdBu', annot=True, ax=ax) \n",
    "\n",
    "#Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf563c-efa3-4854-bead-6754e1e8c808",
   "metadata": {},
   "source": [
    "Now do some exploratory analysis with `long_df`. Let's examine the `OCI` column using `.describe()` and `.hist()`\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Examine <code>long_df</code> using histograms and descriptive statistics by looking specifically at the <code>'OCI'</code> column.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b18e4-e3b9-4905-912e-59d121172e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###\n",
    "long_df['OCI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160669a-8943-4d16-a516-57acdb1ebed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###\n",
    "long_df['OCI'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f0bc37-5398-4c61-bc7e-162a71f89293",
   "metadata": {},
   "source": [
    "You'll notice that the lowest values of OCI are negative one, which isn't a valid observation. We can presume these were used to record that the data is missing so we can filter out these rows.\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise: </b>How many rows of <code>long_df</code> have an OCI value of -1? (Note: This is good practice before dropping rows) Then remove the rows of long_df where OCI is equal to -1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1854926-25b7-4b49-be65-8b1094e5de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###\n",
    "len(long_df[long_df['OCI']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5280a0-ba98-40fe-bd77-504480e9b242",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###\n",
    "long_df = long_df[long_df['OCI']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ca1e3-3398-4160-8fd0-86011004ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check your Work ###\n",
    "assert (long_df['OCI']>=0).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d8325-ce87-4693-a547-b857e98d4487",
   "metadata": {},
   "source": [
    "Next create a new column with the year of the OCI observation. There are a number of ways you could do this but we are going to follow these steps:\n",
    "- Use string slicing to get the last two characters of each record (this will look like `your_column.str[-2:]`)\n",
    "- Cast this string to an integer data type. **Hint:** you can chain this on top of the step before.\n",
    "- Add 2000 to the result.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Create a new <code>'year'</code> column in <code>long_df</code> using the steps outlined above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe5ada-40ae-4374-8745-5c06a9274348",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_df['year'] = long_df['obs_year'].str[-2:].astype(int)+2000 ### Your Code Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bd640-56bc-407c-b40f-0e9221b9a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check that the column was created as expected\n",
    "assert 1200 < len(long_df[long_df['year'] == 2005]['year']) < 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d5f92d-3c99-4d03-98ce-ef1f1114ac60",
   "metadata": {},
   "source": [
    "Now that we have a year column, let's examine the distribution of OCI by year and the number of segments surfaced in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023b43d-7170-4649-92fd-44ce89ab6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create boxplot of OCI\n",
    "long_df.boxplot(column='OCI', by='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f3e4c-4d88-493e-b76e-267e002436c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create barplot of the count of segments from df. Sorting the index assures the years are plotted in order.\n",
    "df['YR_SURF'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcdb8b-1b70-4000-b58d-fe1992982aee",
   "metadata": {},
   "source": [
    "The last thing we need to do before the bonus exercise is to sort the long data frame and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe2ab7-c886-4ece-8f10-2452ca2a7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort long_df by id and year and save it inplace (save the sorted version over long_df)\n",
    "long_df.sort_values(by=['id', 'year'], inplace=True)\n",
    "\n",
    "#Save long_df as a csv to read in later\n",
    "long_df.to_csv('../data/long.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1e97c7-68c2-42b3-bf69-554db0343c1e",
   "metadata": {},
   "source": [
    "## Bonus Exercise\n",
    "Try a challenge! Let's find the number of total miles by region and class then make it a little nicer by pivoting it to a wide table. You'll need to follow a few steps:\n",
    "- Group by class and region to get the sum of miles. (You'll need to add `.reset_index()` at the end for the next step to work)\n",
    "- Use the pandas `DataFrame.pivot()` method. Look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html) for the method to see how it works.\n",
    "\n",
    "You want your final output to look like this:\n",
    "<img src=\"links/Pivot example.png\" alt=\"An axample of a pivoted data frame\" width=\"200\" style=\"display: block; margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57b71b-ace3-479b-b17c-8acd50849794",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###\n",
    "\n",
    "summary = df.groupby(['CLASS', 'REGION']).sum()['MILES'].reset_index()\n",
    "\n",
    "summary.pivot(index='CLASS', columns='REGION', values='MILES')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
