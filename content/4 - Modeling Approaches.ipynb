{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c23c52a1-e225-4283-8d6e-9c0f086694ef",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lab 4: Modeling Approaches\n",
    "Now that we have cleaned our data and done an initial data exploration we can start modeling! In this lab we will create several simple linear regressions to explore basic approaches to modeling and discuss the results.By the end of this lab you will be able to:\n",
    "- Prepare data for modeling\n",
    "- Fit a model and use it for prediction.\n",
    "- Compare the performance of different models.\n",
    "- Describe the differences between some primary types of machine learning algorithms and when to apply them.\n",
    "\n",
    "We will explore models of `OCI` based upon segment age, segment type, and AADT. To that end the data we will be using contains records for each observation of a segment with segments observed over the course of multiple years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5990e5-ab0d-47a6-b3e0-552e9997ad3d",
   "metadata": {},
   "source": [
    "## Additional Data Cleaning\n",
    "While we've already done some data cleaning we can continue tidying up our data and do some additional exploratory analysis to ensure that our data is acurate for use in our model. To do this we will utilize simple visualization and perform some additional data cleaning as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cb8b04-9f00-4acc-8e5a-ba5cb42ada2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only run this code if using Jupyter Lite ###\n",
    "#Installs plotting library\n",
    "%pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fce1106-be8f-4916-868a-136051a95931",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import our data analysis library\n",
    "import pandas as pd\n",
    "#Import a plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "#Import a plotting library useful for styling a plot\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1de8af-b118-499f-bb51-5ec9c3e333e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read our data and visualize the structure\n",
    "df = pd.read_csv('data/lab_4.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82436712-975e-43d6-9f8c-2b997600608d",
   "metadata": {},
   "source": [
    "First things first, we need to devise a column for the age of the segment when it was observed, we can do this simply by calculating a new column. Then let's visualize the relationship between age and OCI using a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd81006-9609-4177-9aaa-f3526d460758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the age relative to the last resurfacing as a new column\n",
    "df['age'] = df['year'] - df['YR_SURF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de50ee-9f4c-44e1-8c17-172b4b8d63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the relationship between age and OCI as a boxplot\n",
    "sns.boxplot(df, x='age', y='OCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39075e0-9ebe-4d30-a6f9-255534321896",
   "metadata": {},
   "source": [
    "What's up with the negative ages? It seems as though segments recieve maintenance about once every eight to ten years and it's likely that there were some inacuracies with how these observations were recorded. Let's modify the age for these records by adjusting them forward to better match the pattern we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab08fd7-79d9-41d1-a413-d86e18f2686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find rows where there is a negative age\n",
    "negative_age_mask = df['age'] < 0\n",
    "#Find the minimum age of the data frame\n",
    "min_age = df['age'].min()\n",
    "\n",
    "#For the rows where age is negative, add the absolute value of the minimum age plus one to the age. This shifts the negative records forward by 9 years. \n",
    "df.loc[negative_age_mask, 'age'] += abs(min_age) + 1\n",
    "\n",
    "#Visualize again to see how our change affects the data.\n",
    "sns.boxplot(df, x='age', y='OCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415f39d-72d1-4801-9d23-f30ab4d0f626",
   "metadata": {},
   "source": [
    "We have a lot of outliers where pavements are considered poor condition. Let's filter these by keeping only rows where `OCI` is greater than 70 minus the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e180ac0-8210-4d99-aba4-2d2d013e8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df to keep only rows where OCI is greater than 70 minus the age\n",
    "df = df[df['OCI'] > 70 - df['age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e22521-b25f-4b0a-9235-cc5d5038c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize again to see how our change affects the data.\n",
    "sns.boxplot(df, x='age', y='OCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69965bed-c873-4436-8b61-f2fe83a27e66",
   "metadata": {},
   "source": [
    "Now this looks like there is a trend we can model! There seems to be something happening to records with an age of zero however. Let's take a closer look with a histogram to see how many records there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2f257-5078-46b6-940e-bdea2e3b04b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a quick histogram of ages\n",
    "df['age'].hist(bins=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aafd7e3-3c23-4976-9138-599e7ac868f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Remove segments where the age is 0.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867a4fc-5d4b-45df-b965-a8df5395dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801d853-97cb-49c3-87d4-3b483a575695",
   "metadata": {},
   "source": [
    "Now that we have our `age` sorted, let's create a new column that records a unique sequence id to differentiate records of a segment before and after it was surfaced. We do this by adding an additional value to the id field based on a logic test that returns true if the year observed is less than or equal to the year surfaced, otherwise it returns false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b49ceb-1f3e-4f07-ae2e-d966c4836296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series of boolean values based on the logic test.\n",
    "sequence = df['year'] <= df['YR_SURF']\n",
    "\n",
    "#Concatenate boolean values (cast to strings of '1' or '0') from sequence with the id field\n",
    "df['seq_id'] = df['id'].astype(str) + '-' + (sequence).astype(int).astype(str)\n",
    "\n",
    "#Look at how this affects our data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91a08b-af4f-4ac0-8e29-f3f12c81bd7c",
   "metadata": {},
   "source": [
    "Now let's keep only rows where there are at least two records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2ab50-1e4e-4e84-8fba-ab1cb9004240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter based upon the length of the groups of sequence id\n",
    "df = df.groupby('seq_id').filter(lambda x: len(x) >= 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b348a9-f46b-48d9-a1eb-180e64334c42",
   "metadata": {},
   "source": [
    "One more cleaning step! There are likely sequences that recieved maintenance but the maintenance may not have been recorded. These records could pose a challenge as we train a model, so let's remove them. To do this we will:\n",
    "- Get the year-over-year change in OCI\n",
    "- Infer that maintenance occured on sequences where the change was greater than 7 points to a score greater than 92 and get the values of the `seq_id`s as a list.\n",
    "- Filter out the these sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e94a5f2-ce2a-4bd6-9a60-0cfe1e1e6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get year-over-year change in OCI\n",
    "df['change'] = df.groupby('id')['OCI'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081240b-aec0-4313-9446-46e0c14510f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer maintenance where condition jumps more than 7 points to > 92 OCI\n",
    "maintenance_seqs = df.loc[(df['change'] > 7) & (df['OCI'] > 92), 'seq_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca4c3d-8562-4b35-b93c-1a1ce01469a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter df to keep only records where seq_id is not in maintenance_seqs\n",
    "df = df[~df['seq_id'].isin(maintenance_seqs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd821b0-e9a4-49c6-9f41-75ccc551d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize again to see how our change affects the data.\n",
    "sns.boxplot(df, x='age', y='OCI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2549d9d-8125-419d-9b85-9d84b2698855",
   "metadata": {},
   "source": [
    "This has helped us get our data much more consistent! Before values for OCI at higher ages (around 14-15) seemed to jump but now they appear much more in line with the consistent trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19c6d6-d612-4a01-909e-b6a01bc6b974",
   "metadata": {},
   "source": [
    "Before we get to modeling let's plot a scatter plot with a line of best fit to see what these trends might look like. Using the seaborn `.lmplot()` method we can even adjust the order of a polynomial line of best fit (where 1 is linear, 2 is quadratic, and so on). We can also differentiate between the surface types to see if there is a meaningful difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec703b-dd52-422f-8816-40696f50facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='age', y='OCI', hue='SURFACE_TY', data=df,     #Define data and variables\n",
    "           order=5,                                         #Change the polynomial order of the line of best fit\n",
    "           scatter_kws={\"alpha\": 0.5, \"s\": 10},             #Change the visual characteristics of the scatter plot\n",
    "           x_jitter=.25                                     #Add some variation to the age values displayed to make a nicer plot\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee0497-0c97-4ecc-93b2-ad832dac1e9b",
   "metadata": {},
   "source": [
    "- What do we learn from this chart? \n",
    "- What might this mean as we begin modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af592660-89a4-4c03-8818-ec849c09c941",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "There are many types of models and many python libraries that can be used for modeling. Two prominent libraries are Scikit Learn and Statsmodels. Today we will focus on Scikit Learn but both are useful. Between the two libraries Scikit Learn offers a wider variety of modeling algorithms including decision trees and neural networks, but Statsmodels offers simplified statistical analysis similar to outputs available in other statistical modeling software such as R or Stata. As such, Statsmodels is better suited for explanatory or inferential analysis while Scikit Learn is better suited for classification or prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62ca0b-393d-447e-94fb-9c4409b6cf4e",
   "metadata": {},
   "source": [
    "For starters, let's import the LinearRegression module from sci-kit learn. We've also utilized a function that will simplify our ability to print some core metrics for our regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945814f-2b25-4f1d-be64-da5835d8c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regression function as well as functions for obtaining metrics about our models.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Create a function for us to get the metrics on our regression models quickly\n",
    "# Source: https://stackoverflow.com/questions/26319259/how-to-get-a-regression-summary-in-scikit-learn-like-r-does\n",
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    # Print regression metrics\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486b71a-a39e-46ba-8425-2dcb06b8c355",
   "metadata": {},
   "source": [
    "Let's start with a very simple linear regression just using `age` as a factor and see what we can do with the regression object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe45fc-e2db-4bd8-a51a-6f762faa8a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our x variables (must be in the form of a list)\n",
    "xvars = ['age']\n",
    "# Define our y variable (must be in the form of a string)\n",
    "yvar = 'OCI'\n",
    "\n",
    "# Create variables for the x and y data\n",
    "x = df[xvars]\n",
    "y = df[yvar]\n",
    "\n",
    "# create a new object with a fit LinearRegression based on our x and y variable\n",
    "reg = LinearRegression().fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ea266-982c-4ac1-b1ff-310088e3be91",
   "metadata": {},
   "source": [
    "Now we have a linear regression object called `reg`. There are a few things we can do with it. For starters let's get the coefficient for our x variable and the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45cc604-9a53-4f82-9db8-d72baadf1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients for the x variables. \n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502874f9-8091-425c-9bba-37c29ca4a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get intercept of regression function\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7503c9-995a-4988-b602-104e48941616",
   "metadata": {},
   "source": [
    "We can also use the model to predict data. More on this later but for now let's predict based on the data we have in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa18cf-a758-4705-8ed7-f1fc0f8b9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to predict values for each value of x\n",
    "# We'll save this to calculate some metrics later on\n",
    "y_pred = reg.predict(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951e409-bca6-436e-981e-8dff624155bb",
   "metadata": {},
   "source": [
    "We can also get some metrics about our model including our coefficient of variation (R-squared) and other measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e232071-08f4-4862-9139-e37aaac3b09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the coefficient of variation (R-squared)\n",
    "reg.score(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090b501-06b7-48aa-ab32-ce814daaf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function defined earlier to get more metrics \n",
    "# based on the y values and our predicted y values\n",
    "regression_results(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cc57f7-798e-48b6-9501-fee9348ea0d2",
   "metadata": {},
   "source": [
    "We can also visualize how effective our model is by graphing the predicted values in relation to the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959f3f0-da7f-4a9f-ab21-6bb4230398ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(5, 5)) \n",
    "\n",
    "#Create the regplot\n",
    "sns.regplot(x=y, y=y_pred, \n",
    "            y_jitter=.35, \n",
    "            scatter_kws={\"alpha\": 0.25, \"s\": 10, 'color':'orange'}, \n",
    "            ax=ax\n",
    "    )\n",
    "\n",
    "# Label both axes more intuitively\n",
    "plt.xlabel('Observed OCI')\n",
    "plt.ylabel('Predicted OCI')\n",
    "\n",
    "# Set the scales on both axes to be equivalent to better visualize model performance\n",
    "ax.set_xlim([60, 100])\n",
    "ax.set_ylim([60, 100])\n",
    "\n",
    "# \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f7c78-dc51-46e9-9ca1-93d6094a4ad3",
   "metadata": {},
   "source": [
    "Now that we have done a simple linear model let's talk talk about expanding this to include other model and variable types and preprocessing.\n",
    "\n",
    "In contrast to data cleaning (where we are ensuring our data is correct, complete, and concise), preprocessing data for modeling is where take our cleaned data and optimize it to the needs of our modeling approach. This may look like:\n",
    "- Encoding categorical variables to binary columns (one hot encoding)\n",
    "- Scaling data (to ensure variables with different units are on the same scale)\n",
    "- Creating interaction or polynomial variables\n",
    "- Spliting training and testing data (to better validate models)\n",
    "\n",
    "Different modeling aproaches require some or all of these techniques. Typically prediction-oriented approaches (such as neural networks and random forests) can be best calibrated when spliting training and testing data to ensure models are not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e2f2c-c2d5-4f4b-a740-b8e1545fdabd",
   "metadata": {},
   "source": [
    "Let's explore some of these techniques as well as a different type of regressor, the Random Forest regressor.\n",
    "We will:\n",
    "- Create an encoded field for surface type uing the pandas `.get_dummies()` function\n",
    "- Define a term with the interaction between age and surface type\n",
    "- Split our data into training and testing data sets\n",
    "- Fit a random forest regression model\n",
    "- Evaluate our model using our testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e5c61-efcc-4d21-8f5c-73eb94f37282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dummy column of surface type (this will return column of zeros and ones based on the surface type\n",
    "df_encoded = pd.get_dummies(df, columns=['SURFACE_TY'], drop_first=True, dtype='int')\n",
    "\n",
    "#Create interaction term by multiplying age by surface type \n",
    "df_encoded['interaction'] = df_encoded['age']*df_encoded['SURFACE_TY_Concrete']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039cbfc-752c-4ad2-a7f5-d9dc4f167d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import splitting and regression function.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec08e5-9ce5-4287-8ee7-03f0d42d0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our x and y variables\n",
    "xvars = ['age', 'interaction']\n",
    "yvar = 'OCI'\n",
    "\n",
    "# Split the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_encoded[xvars],      # Define x variables\n",
    "    df_encoded[yvar],       # Define y variables\n",
    "    test_size = 0.25,       # Define the proporiton of the data used for testing\n",
    "    random_state = 1        # Set a random state so our results are repeatable\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925e2d33-b956-47f6-975d-f2af2af654c5",
   "metadata": {},
   "source": [
    "Now that our data is preprocessed let's fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aad8a3-0550-4354-a1a7-299e392e5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random forest object and fit using our training data.\n",
    "rf = RandomForestRegressor(max_depth=2, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c0cc1-42bb-4f3a-aeef-eff9367b0ec7",
   "metadata": {},
   "source": [
    "Note, random forest models have a wide variety of hyperparameters (modifiable model characteristics), such as the `max_depth` characteristic shown here that can greatly influence how well they perform. Delving too deep into these is beyond the scope of this workshop but see what happens when you change the `max_depth` or look on the sci-kit learn documentation to see what other hyperparameters you can adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e40e12-8220-4640-9d3c-874bada7c1e9",
   "metadata": {},
   "source": [
    "Now that we have a model that has been fit, let's evaluate it by creating a prediction on our testing data and evaluating the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d7958-5ff1-42b6-b75c-26db4b59ccf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of predictions using our testing data\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbac84-0689-4824-9b70-b2257c878446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at our model metrics\n",
    "regression_results(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3add64a-3e60-4522-b021-1923fb8665dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the observed and predicted values of our testing data ###\n",
    "\n",
    "#Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(5, 5)) \n",
    "\n",
    "#Create the regplot\n",
    "sns.regplot(x=y_test, y=y_pred, \n",
    "            y_jitter=.35, \n",
    "            scatter_kws={\"alpha\": 0.25, \"s\": 10, 'color':'orange'}, \n",
    "            ax=ax\n",
    "    )\n",
    "\n",
    "# Label both axes more intuitively\n",
    "plt.xlabel('Observed OCI')\n",
    "plt.ylabel('Predicted OCI')\n",
    "\n",
    "# Set the scales on both axes to be equivalent to better visualize model performance\n",
    "ax.set_xlim([60, 100])\n",
    "ax.set_ylim([60, 100])\n",
    "\n",
    "# \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0757a-873a-464a-8373-28652b751887",
   "metadata": {},
   "source": [
    "How does this model compare to the simple linear regression? Does it perform better or worse? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3761b7-a302-442f-9b81-5afa7fbec140",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "Now that you have seen an example of both a linear regression and a random forest regression it's time for you to give it a try.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Exercise:</b> Create two more models and see how well they perform in relation to these other models by looking at the metrics of their performance. \n",
    "<ul>Create a\n",
    "<li>Linear Model with <code>age</code> and <code>interaction</code> as variables.</li>\n",
    "<li>Either a linear or random forest model that includes both <code>age</code> and <code>interaction</code> with the addition of <code>AADT</code>.</li>\n",
    "</ul>\n",
    "Remember random forest models require you to split training and testing data. After creating and assessing both models briefly describe which of the models you think performs best and why.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef542c41-f7b4-40c2-bb94-5bef279af914",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77233e35-3921-47bb-9df4-c877617ab199",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7ce50-0474-4bbd-b9e9-49f2619b70be",
   "metadata": {},
   "source": [
    "Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbb367-87a8-48a1-bf6d-bd1fc786d31f",
   "metadata": {},
   "source": [
    "## Bonus Exercise:\n",
    "Pick any or all of the following as a bonus exercise! Feel free to use online resources such as the documentation and stack overflow if you find yourself getting stuck.\n",
    "- Create a higher order variable with `age` such as `age^2` and/or `age^3` and add it to a linear model. Briefly describe the performance of the model in relation to others you've created.\n",
    "- Create a Random Forest Model with the addition of the encoded surface type column (`SURFACE_TY_Concrete`).\n",
    "- Explore the hyperparameters of a Random Forest Model and see how they affect the model performance.\n",
    "- The plot we used to visualize model performance seems pretty standardized. Turn this into a function and use it while evaluating one of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb494158-5681-4ff8-8981-65a30b19b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9d449-8541-43ff-bc30-8637e725dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
